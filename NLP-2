# LAB/HOMEWORK.
# BMI 500; WEEK 6: NATURAL LANGUAGE PROCESSING 2
# AUTHOR: ABEED SARKER
# DATE: 09/2022

# SUBMISSION PROTOCOL:
# Put all code in a .py file.
# Prepare a report for the answers to the following questions.
# Submit both the .py files and the report.

#imports
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import jaccard_score
import numpy as np

# 1. You are provided two text files (text1 and text2)
# a) Preprocess the two texts by lowercasing them.

corpus1 = open('/Users/elizabethnemetichipkes/Desktop/text1', 'rt').read().lower() #rt for reading (r) text mode (t)
corpus2 = open('/Users/elizabethnemetichipkes/Desktop/text2', 'rt').read().lower()
texts = [corpus1, corpus2]

# corpus1 = [corpus1] # to fix error of string instead of raw data expected
# corpus2 = [corpus2]

# b) Follow the instructions on https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
# to vectorize the two texts using the CountVectorizer. Use n-gram size of 1-3 for the vectorizer.

vectorizer = CountVectorizer(ngram_range=(1,3)) #creates vectorizer
vectorized_texts = vectorizer.fit_transform(texts) # Learn the vocabulary dictionary and return document-term matrix
print(vectorizer.get_feature_names_out([texts])) # Get output feature names for transformation
vectorized_text1 = vectorized_texts[0] # index first vectorized text
vectorized_text2 = vectorized_texts[1] # index second vectorized text
print(vectorized_text1.toarray()) # transform vectorized texts
print(vectorized_text2.toarray())

# c) Compute the cosine similarity (a metric that allows you to measure the similarity of the documents) between the texts (sklearn provides a function for it).
print(cosine_similarity(vectorized_text1.toarray(), vectorized_text2.toarray()))

# d) Compute the jaccard similarity between text1 and text2 (measures the similarity between two sets of data to see which members are shared and distinct)
print(jaccard_score(vectorized_text1.toarray()[0], vectorized_text2.toarray()[0], average='macro')) # macro for multilabel case

# 2. Read section 4 and 5 of the nltk book: https://www.nltk.org/book/ch05.html.
# a) Follow the instructions to train a POS (part of speech) tagger.
# Show the performance of different taggers (e.g., unigram, bigram and combined taggers) on your chosen corpus (e.g. the brown corpus).
# Use 90% of the data for training and 10% for evaluation.
# Compare the performances of at least 3 taggers.
# Save and load the tagger.
# Use a trained tagger to tag all the words from text1.

import nltk
from nltk.corpus import brown

# load the data
brown_tagged_sents = brown.tagged_sents(categories='news') # load tagged sentences

# Separating the Training and Testing Data
training_size = int(len(brown_tagged_sents) * 0.9) # training on 90% of the data
print(training_size) # 4160
training_sents = brown_tagged_sents[:training_size]
test_sents = brown_tagged_sents[training_size:]

# # unigram POS
# unigram_tagger = nltk.UnigramTagger(training_sents) # training
# print(unigram_tagger.evaluate(test_sents)) # testing/ evaluation
# # 0.8121
#
# # bigram POS
# bigram_tagger = nltk.BigramTagger(training_sents) # training
# print(bigram_tagger.evaluate(test_sents)) # testing/ evaluation
# # 0.1021
#
# # combined tagger for default POS, unigram POS, bigram POS
# t0 = nltk.DefaultTagger('NN') # If the unigram tagger is also unable to find a tag, use a default tagger
# t1 = nltk.UnigramTagger(training_sents, backoff=t0) # If the bigram tagger is unable to find a tag for the token, try the unigram tagger
# t2 = nltk.BigramTagger(training_sents, backoff=t1) # Try tagging the token with the bigram tagger
# print(t2.evaluate(test_sents))
# # 0.8452

# Can lowercasing affect the performance of the POS tagger? (Put your answer as comment in python file)
# unigram POS
unigram_tagger = nltk.UnigramTagger(training_sents.lower()) # training
print(unigram_tagger.evaluate(test_sents.lower())) # testing/ evaluation
# 0.8121

# bigram POS
bigram_tagger = nltk.BigramTagger(training_sents.lower()) # training
print(bigram_tagger.evaluate(test_sents.lower())) # testing/ evaluation
# 0.1021

